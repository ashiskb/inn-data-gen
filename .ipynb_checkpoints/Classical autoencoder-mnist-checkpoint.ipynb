{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from architecture import MNIST_autoencoder as mnist\n",
    "from functionalities import filemanager as fm\n",
    "from functionalities import dataloader as dl\n",
    "from functionalities import gpu \n",
    "from functionalities import plot as pl\n",
    "from functionalities import trainer as tr\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "batch_size = 128\n",
    "lr_init = 1e-3\n",
    "milestones = [10 * x for x in range(1, 11)]\n",
    "latent_dim_lst = [1, 2, 3, 4, 6, 8, 12, 16, 24, 32, 48, 64]\n",
    "number_dev = 0\n",
    "get_model = mnist.mnist_autoencoder\n",
    "modelname = \"mnist_classic_bottleneck\"\n",
    "\n",
    "device = gpu.get_device(number_dev)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset, classes = dl.load_mnist()\n",
    "trainloader, validloader, testloader = dl.make_dataloaders(trainset, testset, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottleneck dimension: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abx4kb/Documents/GITs/nsf_eager_24/malgorzata_workspace/inn-data-gen/functionalities/trainer.py:477: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(m.weight)\n",
      "/home/abx4kb/.conda/envs/venv-p310-torch210-abx4kb/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], train loss:0.1063\n",
      "test loss:0.1118\n",
      "epoch [2/100], train loss:0.1086\n",
      "test loss:0.1080\n",
      "epoch [3/100], train loss:0.1017\n",
      "test loss:0.1040\n",
      "epoch [4/100], train loss:0.0990\n",
      "test loss:0.1015\n",
      "epoch [5/100], train loss:0.0985\n",
      "test loss:0.1000\n",
      "epoch [6/100], train loss:0.0954\n",
      "test loss:0.0994\n",
      "epoch [7/100], train loss:0.1015\n",
      "test loss:0.0977\n",
      "epoch [8/100], train loss:0.0972\n",
      "test loss:0.0966\n",
      "epoch [9/100], train loss:0.0976\n",
      "test loss:0.0958\n",
      "epoch [10/100], train loss:0.0980\n",
      "test loss:0.0933\n",
      "epoch [11/100], train loss:0.0895\n",
      "test loss:0.0931\n",
      "epoch [12/100], train loss:0.0921\n",
      "test loss:0.0929\n",
      "epoch [13/100], train loss:0.0974\n",
      "test loss:0.0926\n",
      "epoch [14/100], train loss:0.0989\n",
      "test loss:0.0926\n",
      "epoch [15/100], train loss:0.0935\n",
      "test loss:0.0924\n",
      "epoch [16/100], train loss:0.0951\n",
      "test loss:0.0922\n",
      "epoch [17/100], train loss:0.0922\n",
      "test loss:0.0921\n",
      "epoch [18/100], train loss:0.0902\n",
      "test loss:0.0920\n",
      "epoch [19/100], train loss:0.0924\n",
      "test loss:0.0919\n",
      "epoch [20/100], train loss:0.0935\n",
      "test loss:0.0916\n",
      "epoch [21/100], train loss:0.0897\n",
      "test loss:0.0916\n",
      "epoch [22/100], train loss:0.0910\n",
      "test loss:0.0916\n",
      "epoch [23/100], train loss:0.0924\n",
      "test loss:0.0916\n",
      "epoch [24/100], train loss:0.0856\n",
      "test loss:0.0915\n",
      "epoch [25/100], train loss:0.0884\n",
      "test loss:0.0915\n",
      "epoch [26/100], train loss:0.0891\n",
      "test loss:0.0915\n",
      "epoch [27/100], train loss:0.0914\n",
      "test loss:0.0915\n",
      "epoch [28/100], train loss:0.0882\n",
      "test loss:0.0915\n",
      "epoch [29/100], train loss:0.0923\n",
      "test loss:0.0915\n",
      "epoch [30/100], train loss:0.0882\n",
      "test loss:0.0915\n",
      "epoch [31/100], train loss:0.0931\n",
      "test loss:0.0915\n",
      "epoch [32/100], train loss:0.0900\n",
      "test loss:0.0915\n",
      "epoch [33/100], train loss:0.0898\n",
      "test loss:0.0915\n",
      "epoch [34/100], train loss:0.0882\n",
      "test loss:0.0915\n",
      "epoch [35/100], train loss:0.0919\n",
      "test loss:0.0915\n",
      "epoch [36/100], train loss:0.0932\n",
      "test loss:0.0915\n",
      "epoch [37/100], train loss:0.0916\n",
      "test loss:0.0915\n",
      "epoch [38/100], train loss:0.0888\n",
      "test loss:0.0915\n",
      "epoch [39/100], train loss:0.0912\n",
      "test loss:0.0915\n",
      "epoch [40/100], train loss:0.0950\n",
      "test loss:0.0915\n",
      "epoch [41/100], train loss:0.0900\n",
      "test loss:0.0915\n",
      "epoch [42/100], train loss:0.0936\n",
      "test loss:0.0915\n",
      "epoch [43/100], train loss:0.0919\n",
      "test loss:0.0915\n",
      "epoch [44/100], train loss:0.0889\n",
      "test loss:0.0915\n",
      "epoch [45/100], train loss:0.0859\n",
      "test loss:0.0915\n",
      "epoch [46/100], train loss:0.0845\n",
      "test loss:0.0915\n",
      "epoch [47/100], train loss:0.0958\n",
      "test loss:0.0915\n",
      "epoch [48/100], train loss:0.0891\n",
      "test loss:0.0915\n",
      "epoch [49/100], train loss:0.0921\n",
      "test loss:0.0915\n",
      "epoch [50/100], train loss:0.0880\n",
      "test loss:0.0915\n",
      "epoch [51/100], train loss:0.0935\n",
      "test loss:0.0915\n",
      "epoch [52/100], train loss:0.0923\n",
      "test loss:0.0915\n",
      "epoch [53/100], train loss:0.0973\n",
      "test loss:0.0915\n",
      "epoch [54/100], train loss:0.0973\n",
      "test loss:0.0915\n",
      "epoch [55/100], train loss:0.0901\n",
      "test loss:0.0915\n",
      "epoch [56/100], train loss:0.0931\n",
      "test loss:0.0915\n",
      "epoch [57/100], train loss:0.0835\n",
      "test loss:0.0915\n",
      "epoch [58/100], train loss:0.0948\n",
      "test loss:0.0915\n",
      "epoch [59/100], train loss:0.0905\n",
      "test loss:0.0915\n",
      "epoch [60/100], train loss:0.0905\n",
      "test loss:0.0915\n",
      "epoch [61/100], train loss:0.0913\n",
      "test loss:0.0915\n",
      "epoch [62/100], train loss:0.0922\n",
      "test loss:0.0915\n",
      "epoch [63/100], train loss:0.0875\n",
      "test loss:0.0915\n",
      "epoch [64/100], train loss:0.0927\n",
      "test loss:0.0915\n",
      "epoch [65/100], train loss:0.0897\n",
      "test loss:0.0915\n",
      "epoch [66/100], train loss:0.0902\n",
      "test loss:0.0915\n",
      "epoch [67/100], train loss:0.0903\n",
      "test loss:0.0915\n",
      "epoch [68/100], train loss:0.0861\n",
      "test loss:0.0915\n",
      "epoch [69/100], train loss:0.0967\n",
      "test loss:0.0915\n",
      "epoch [70/100], train loss:0.0915\n",
      "test loss:0.0915\n",
      "epoch [71/100], train loss:0.0929\n",
      "test loss:0.0915\n",
      "epoch [72/100], train loss:0.0894\n",
      "test loss:0.0915\n",
      "epoch [73/100], train loss:0.0911\n",
      "test loss:0.0915\n",
      "epoch [74/100], train loss:0.0924\n",
      "test loss:0.0915\n",
      "epoch [75/100], train loss:0.0912\n",
      "test loss:0.0915\n",
      "epoch [76/100], train loss:0.0857\n",
      "test loss:0.0915\n",
      "epoch [77/100], train loss:0.0889\n",
      "test loss:0.0915\n",
      "epoch [78/100], train loss:0.0906\n",
      "test loss:0.0915\n",
      "epoch [79/100], train loss:0.0929\n",
      "test loss:0.0915\n",
      "epoch [80/100], train loss:0.0905\n",
      "test loss:0.0915\n",
      "epoch [81/100], train loss:0.0923\n",
      "test loss:0.0915\n",
      "epoch [82/100], train loss:0.0899\n",
      "test loss:0.0915\n",
      "epoch [83/100], train loss:0.0935\n",
      "test loss:0.0915\n",
      "epoch [84/100], train loss:0.0933\n",
      "test loss:0.0915\n",
      "epoch [85/100], train loss:0.0921\n",
      "test loss:0.0915\n",
      "epoch [86/100], train loss:0.0907\n",
      "test loss:0.0915\n",
      "epoch [87/100], train loss:0.0893\n",
      "test loss:0.0915\n",
      "epoch [88/100], train loss:0.0920\n",
      "test loss:0.0915\n",
      "epoch [89/100], train loss:0.0915\n",
      "test loss:0.0915\n",
      "epoch [90/100], train loss:0.0901\n",
      "test loss:0.0915\n",
      "epoch [91/100], train loss:0.0924\n",
      "test loss:0.0915\n",
      "epoch [92/100], train loss:0.0898\n",
      "test loss:0.0915\n",
      "epoch [93/100], train loss:0.0971\n",
      "test loss:0.0915\n",
      "epoch [94/100], train loss:0.0869\n",
      "test loss:0.0915\n",
      "epoch [95/100], train loss:0.0898\n",
      "test loss:0.0915\n",
      "epoch [96/100], train loss:0.0889\n",
      "test loss:0.0915\n",
      "epoch [97/100], train loss:0.0888\n",
      "test loss:0.0915\n",
      "epoch [98/100], train loss:0.0942\n",
      "test loss:0.0915\n",
      "epoch [99/100], train loss:0.0936\n",
      "test loss:0.0915\n",
      "epoch [100/100], train loss:0.0999\n",
      "test loss:0.0915\n",
      "bottleneck dimension: 2\n",
      "epoch [1/100], train loss:0.1016\n",
      "test loss:0.0992\n",
      "epoch [2/100], train loss:0.0890\n",
      "test loss:0.0908\n",
      "epoch [3/100], train loss:0.0900\n",
      "test loss:0.0875\n",
      "epoch [4/100], train loss:0.0860\n",
      "test loss:0.0863\n",
      "epoch [5/100], train loss:0.0829\n",
      "test loss:0.0845\n",
      "epoch [6/100], train loss:0.0839\n",
      "test loss:0.0838\n",
      "epoch [7/100], train loss:0.0851\n",
      "test loss:0.0823\n",
      "epoch [8/100], train loss:0.0813\n",
      "test loss:0.0817\n",
      "epoch [9/100], train loss:0.0766\n",
      "test loss:0.0818\n",
      "epoch [10/100], train loss:0.0772\n",
      "test loss:0.0786\n",
      "epoch [11/100], train loss:0.0786\n",
      "test loss:0.0785\n",
      "epoch [12/100], train loss:0.0791\n",
      "test loss:0.0782\n",
      "epoch [13/100], train loss:0.0778\n",
      "test loss:0.0780\n",
      "epoch [14/100], train loss:0.0800\n",
      "test loss:0.0779\n",
      "epoch [15/100], train loss:0.0780\n",
      "test loss:0.0778\n",
      "epoch [16/100], train loss:0.0812\n",
      "test loss:0.0777\n",
      "epoch [17/100], train loss:0.0780\n",
      "test loss:0.0776\n",
      "epoch [18/100], train loss:0.0766\n",
      "test loss:0.0775\n",
      "epoch [19/100], train loss:0.0741\n",
      "test loss:0.0773\n",
      "epoch [20/100], train loss:0.0761\n",
      "test loss:0.0771\n",
      "epoch [21/100], train loss:0.0780\n",
      "test loss:0.0771\n",
      "epoch [22/100], train loss:0.0708\n",
      "test loss:0.0771\n",
      "epoch [23/100], train loss:0.0768\n",
      "test loss:0.0770\n",
      "epoch [24/100], train loss:0.0724\n",
      "test loss:0.0770\n",
      "epoch [25/100], train loss:0.0824\n",
      "test loss:0.0770\n",
      "epoch [26/100], train loss:0.0729\n",
      "test loss:0.0770\n",
      "epoch [27/100], train loss:0.0807\n",
      "test loss:0.0770\n",
      "epoch [28/100], train loss:0.0778\n",
      "test loss:0.0770\n",
      "epoch [29/100], train loss:0.0787\n",
      "test loss:0.0770\n",
      "epoch [30/100], train loss:0.0809\n",
      "test loss:0.0770\n",
      "epoch [31/100], train loss:0.0761\n",
      "test loss:0.0770\n",
      "epoch [32/100], train loss:0.0771\n",
      "test loss:0.0770\n",
      "epoch [33/100], train loss:0.0768\n",
      "test loss:0.0770\n",
      "epoch [34/100], train loss:0.0759\n",
      "test loss:0.0770\n",
      "epoch [35/100], train loss:0.0767\n",
      "test loss:0.0770\n",
      "epoch [36/100], train loss:0.0747\n",
      "test loss:0.0770\n",
      "epoch [37/100], train loss:0.0808\n",
      "test loss:0.0770\n",
      "epoch [38/100], train loss:0.0764\n",
      "test loss:0.0770\n",
      "epoch [39/100], train loss:0.0755\n",
      "test loss:0.0770\n",
      "epoch [40/100], train loss:0.0750\n",
      "test loss:0.0770\n",
      "epoch [41/100], train loss:0.0767\n",
      "test loss:0.0770\n",
      "epoch [42/100], train loss:0.0777\n",
      "test loss:0.0770\n",
      "epoch [43/100], train loss:0.0748\n",
      "test loss:0.0770\n",
      "epoch [44/100], train loss:0.0752\n",
      "test loss:0.0770\n",
      "epoch [45/100], train loss:0.0764\n",
      "test loss:0.0770\n",
      "epoch [46/100], train loss:0.0770\n",
      "test loss:0.0769\n",
      "epoch [47/100], train loss:0.0743\n",
      "test loss:0.0769\n",
      "epoch [48/100], train loss:0.0752\n",
      "test loss:0.0769\n",
      "epoch [49/100], train loss:0.0730\n",
      "test loss:0.0769\n",
      "epoch [50/100], train loss:0.0748\n",
      "test loss:0.0769\n",
      "epoch [51/100], train loss:0.0748\n",
      "test loss:0.0769\n",
      "epoch [52/100], train loss:0.0763\n",
      "test loss:0.0769\n",
      "epoch [53/100], train loss:0.0761\n",
      "test loss:0.0769\n",
      "epoch [54/100], train loss:0.0756\n",
      "test loss:0.0769\n",
      "epoch [55/100], train loss:0.0814\n",
      "test loss:0.0769\n",
      "epoch [56/100], train loss:0.0743\n",
      "test loss:0.0769\n",
      "epoch [57/100], train loss:0.0751\n",
      "test loss:0.0769\n",
      "epoch [58/100], train loss:0.0767\n",
      "test loss:0.0769\n",
      "epoch [59/100], train loss:0.0799\n",
      "test loss:0.0769\n",
      "epoch [60/100], train loss:0.0806\n",
      "test loss:0.0769\n",
      "epoch [61/100], train loss:0.0708\n",
      "test loss:0.0769\n",
      "epoch [62/100], train loss:0.0696\n",
      "test loss:0.0769\n",
      "epoch [63/100], train loss:0.0790\n",
      "test loss:0.0769\n",
      "epoch [64/100], train loss:0.0805\n",
      "test loss:0.0769\n",
      "epoch [65/100], train loss:0.0783\n",
      "test loss:0.0769\n",
      "epoch [66/100], train loss:0.0745\n",
      "test loss:0.0769\n",
      "epoch [67/100], train loss:0.0790\n",
      "test loss:0.0769\n",
      "epoch [68/100], train loss:0.0734\n",
      "test loss:0.0769\n",
      "epoch [69/100], train loss:0.0748\n",
      "test loss:0.0769\n",
      "epoch [70/100], train loss:0.0762\n",
      "test loss:0.0769\n",
      "epoch [71/100], train loss:0.0763\n",
      "test loss:0.0769\n",
      "epoch [72/100], train loss:0.0805\n",
      "test loss:0.0769\n",
      "epoch [73/100], train loss:0.0751\n",
      "test loss:0.0769\n",
      "epoch [74/100], train loss:0.0792\n",
      "test loss:0.0769\n",
      "epoch [75/100], train loss:0.0755\n",
      "test loss:0.0769\n",
      "epoch [76/100], train loss:0.0790\n",
      "test loss:0.0769\n",
      "epoch [77/100], train loss:0.0762\n",
      "test loss:0.0769\n",
      "epoch [78/100], train loss:0.0750\n",
      "test loss:0.0769\n",
      "epoch [79/100], train loss:0.0754\n",
      "test loss:0.0769\n",
      "epoch [80/100], train loss:0.0746\n",
      "test loss:0.0769\n",
      "epoch [81/100], train loss:0.0744\n",
      "test loss:0.0769\n",
      "epoch [82/100], train loss:0.0816\n",
      "test loss:0.0769\n",
      "epoch [83/100], train loss:0.0834\n",
      "test loss:0.0769\n",
      "epoch [84/100], train loss:0.0764\n",
      "test loss:0.0769\n",
      "epoch [85/100], train loss:0.0743\n",
      "test loss:0.0769\n",
      "epoch [86/100], train loss:0.0724\n",
      "test loss:0.0769\n",
      "epoch [87/100], train loss:0.0742\n",
      "test loss:0.0769\n",
      "epoch [88/100], train loss:0.0771\n",
      "test loss:0.0769\n",
      "epoch [89/100], train loss:0.0731\n",
      "test loss:0.0769\n",
      "epoch [90/100], train loss:0.0764\n",
      "test loss:0.0769\n",
      "epoch [91/100], train loss:0.0735\n",
      "test loss:0.0769\n",
      "epoch [92/100], train loss:0.0786\n",
      "test loss:0.0769\n",
      "epoch [93/100], train loss:0.0735\n",
      "test loss:0.0769\n",
      "epoch [94/100], train loss:0.0760\n",
      "test loss:0.0769\n",
      "epoch [95/100], train loss:0.0767\n",
      "test loss:0.0769\n",
      "epoch [96/100], train loss:0.0741\n",
      "test loss:0.0769\n",
      "epoch [97/100], train loss:0.0776\n",
      "test loss:0.0769\n",
      "epoch [98/100], train loss:0.0781\n",
      "test loss:0.0769\n",
      "epoch [99/100], train loss:0.0735\n",
      "test loss:0.0769\n",
      "epoch [100/100], train loss:0.0757\n",
      "test loss:0.0769\n",
      "bottleneck dimension: 3\n",
      "epoch [1/100], train loss:0.0871\n",
      "test loss:0.0926\n",
      "epoch [2/100], train loss:0.0810\n",
      "test loss:0.0824\n",
      "epoch [3/100], train loss:0.0782\n",
      "test loss:0.0796\n",
      "epoch [4/100], train loss:0.0760\n",
      "test loss:0.0777\n",
      "epoch [5/100], train loss:0.0749\n",
      "test loss:0.0755\n",
      "epoch [6/100], train loss:0.0711\n",
      "test loss:0.0745\n",
      "epoch [7/100], train loss:0.0769\n",
      "test loss:0.0735\n",
      "epoch [8/100], train loss:0.0762\n",
      "test loss:0.0725\n",
      "epoch [9/100], train loss:0.0717\n",
      "test loss:0.0722\n",
      "epoch [10/100], train loss:0.0668\n",
      "test loss:0.0692\n",
      "epoch [11/100], train loss:0.0684\n",
      "test loss:0.0690\n",
      "epoch [12/100], train loss:0.0682\n",
      "test loss:0.0688\n",
      "epoch [13/100], train loss:0.0686\n",
      "test loss:0.0687\n",
      "epoch [14/100], train loss:0.0669\n",
      "test loss:0.0685\n",
      "epoch [15/100], train loss:0.0671\n",
      "test loss:0.0685\n",
      "epoch [16/100], train loss:0.0661\n",
      "test loss:0.0683\n",
      "epoch [17/100], train loss:0.0662\n",
      "test loss:0.0682\n",
      "epoch [18/100], train loss:0.0690\n",
      "test loss:0.0682\n",
      "epoch [19/100], train loss:0.0644\n",
      "test loss:0.0681\n",
      "epoch [20/100], train loss:0.0652\n",
      "test loss:0.0678\n",
      "epoch [21/100], train loss:0.0696\n",
      "test loss:0.0677\n",
      "epoch [22/100], train loss:0.0669\n",
      "test loss:0.0677\n",
      "epoch [23/100], train loss:0.0677\n",
      "test loss:0.0677\n",
      "epoch [24/100], train loss:0.0637\n",
      "test loss:0.0677\n",
      "epoch [25/100], train loss:0.0687\n",
      "test loss:0.0677\n",
      "epoch [26/100], train loss:0.0675\n",
      "test loss:0.0677\n",
      "epoch [27/100], train loss:0.0686\n",
      "test loss:0.0677\n",
      "epoch [28/100], train loss:0.0683\n",
      "test loss:0.0676\n",
      "epoch [29/100], train loss:0.0689\n",
      "test loss:0.0676\n",
      "epoch [30/100], train loss:0.0675\n",
      "test loss:0.0676\n",
      "epoch [31/100], train loss:0.0655\n",
      "test loss:0.0676\n",
      "epoch [32/100], train loss:0.0698\n",
      "test loss:0.0676\n",
      "epoch [33/100], train loss:0.0636\n",
      "test loss:0.0676\n",
      "epoch [34/100], train loss:0.0726\n",
      "test loss:0.0676\n",
      "epoch [35/100], train loss:0.0649\n",
      "test loss:0.0676\n",
      "epoch [36/100], train loss:0.0669\n",
      "test loss:0.0676\n",
      "epoch [37/100], train loss:0.0667\n",
      "test loss:0.0676\n",
      "epoch [38/100], train loss:0.0647\n",
      "test loss:0.0676\n",
      "epoch [39/100], train loss:0.0645\n",
      "test loss:0.0676\n",
      "epoch [40/100], train loss:0.0627\n",
      "test loss:0.0676\n",
      "epoch [41/100], train loss:0.0658\n",
      "test loss:0.0676\n",
      "epoch [42/100], train loss:0.0673\n",
      "test loss:0.0676\n",
      "epoch [43/100], train loss:0.0685\n",
      "test loss:0.0676\n",
      "epoch [44/100], train loss:0.0677\n",
      "test loss:0.0676\n",
      "epoch [45/100], train loss:0.0705\n",
      "test loss:0.0676\n",
      "epoch [46/100], train loss:0.0667\n",
      "test loss:0.0676\n",
      "epoch [47/100], train loss:0.0663\n",
      "test loss:0.0676\n",
      "epoch [48/100], train loss:0.0675\n",
      "test loss:0.0676\n",
      "epoch [49/100], train loss:0.0661\n",
      "test loss:0.0676\n",
      "epoch [50/100], train loss:0.0677\n",
      "test loss:0.0676\n",
      "epoch [51/100], train loss:0.0705\n",
      "test loss:0.0676\n",
      "epoch [52/100], train loss:0.0662\n",
      "test loss:0.0676\n",
      "epoch [53/100], train loss:0.0664\n",
      "test loss:0.0676\n",
      "epoch [54/100], train loss:0.0710\n",
      "test loss:0.0676\n",
      "epoch [55/100], train loss:0.0715\n",
      "test loss:0.0676\n",
      "epoch [56/100], train loss:0.0648\n",
      "test loss:0.0676\n",
      "epoch [57/100], train loss:0.0688\n",
      "test loss:0.0676\n",
      "epoch [58/100], train loss:0.0685\n",
      "test loss:0.0676\n",
      "epoch [59/100], train loss:0.0668\n",
      "test loss:0.0676\n",
      "epoch [60/100], train loss:0.0650\n",
      "test loss:0.0676\n",
      "epoch [61/100], train loss:0.0662\n",
      "test loss:0.0676\n",
      "epoch [62/100], train loss:0.0660\n",
      "test loss:0.0676\n",
      "epoch [63/100], train loss:0.0700\n",
      "test loss:0.0676\n",
      "epoch [64/100], train loss:0.0687\n",
      "test loss:0.0676\n",
      "epoch [65/100], train loss:0.0705\n",
      "test loss:0.0676\n",
      "epoch [66/100], train loss:0.0686\n",
      "test loss:0.0676\n",
      "epoch [67/100], train loss:0.0691\n",
      "test loss:0.0676\n",
      "epoch [68/100], train loss:0.0672\n",
      "test loss:0.0676\n",
      "epoch [69/100], train loss:0.0661\n",
      "test loss:0.0676\n",
      "epoch [70/100], train loss:0.0684\n",
      "test loss:0.0676\n",
      "epoch [71/100], train loss:0.0651\n",
      "test loss:0.0676\n",
      "epoch [72/100], train loss:0.0662\n",
      "test loss:0.0676\n",
      "epoch [73/100], train loss:0.0697\n",
      "test loss:0.0676\n",
      "epoch [74/100], train loss:0.0657\n",
      "test loss:0.0676\n",
      "epoch [75/100], train loss:0.0665\n",
      "test loss:0.0676\n",
      "epoch [76/100], train loss:0.0680\n",
      "test loss:0.0676\n",
      "epoch [77/100], train loss:0.0660\n",
      "test loss:0.0676\n",
      "epoch [78/100], train loss:0.0664\n",
      "test loss:0.0676\n",
      "epoch [79/100], train loss:0.0659\n",
      "test loss:0.0676\n",
      "epoch [80/100], train loss:0.0643\n",
      "test loss:0.0676\n",
      "epoch [81/100], train loss:0.0718\n",
      "test loss:0.0676\n",
      "epoch [82/100], train loss:0.0692\n",
      "test loss:0.0676\n",
      "epoch [83/100], train loss:0.0663\n",
      "test loss:0.0676\n",
      "epoch [84/100], train loss:0.0681\n",
      "test loss:0.0676\n",
      "epoch [85/100], train loss:0.0700\n",
      "test loss:0.0676\n",
      "epoch [86/100], train loss:0.0667\n",
      "test loss:0.0676\n",
      "epoch [87/100], train loss:0.0672\n",
      "test loss:0.0676\n",
      "epoch [88/100], train loss:0.0695\n",
      "test loss:0.0676\n",
      "epoch [89/100], train loss:0.0646\n",
      "test loss:0.0676\n",
      "epoch [90/100], train loss:0.0686\n",
      "test loss:0.0676\n",
      "epoch [91/100], train loss:0.0687\n",
      "test loss:0.0676\n",
      "epoch [92/100], train loss:0.0649\n",
      "test loss:0.0676\n",
      "epoch [93/100], train loss:0.0669\n",
      "test loss:0.0676\n",
      "epoch [94/100], train loss:0.0648\n",
      "test loss:0.0676\n",
      "epoch [95/100], train loss:0.0622\n",
      "test loss:0.0676\n",
      "epoch [96/100], train loss:0.0681\n",
      "test loss:0.0676\n",
      "epoch [97/100], train loss:0.0704\n",
      "test loss:0.0676\n",
      "epoch [98/100], train loss:0.0662\n",
      "test loss:0.0676\n",
      "epoch [99/100], train loss:0.0634\n",
      "test loss:0.0676\n",
      "epoch [100/100], train loss:0.0625\n",
      "test loss:0.0676\n",
      "bottleneck dimension: 4\n",
      "epoch [1/100], train loss:0.0958\n",
      "test loss:0.0940\n",
      "epoch [2/100], train loss:0.0833\n",
      "test loss:0.0804\n",
      "epoch [3/100], train loss:0.0779\n",
      "test loss:0.0757\n",
      "epoch [4/100], train loss:0.0732\n",
      "test loss:0.0730\n",
      "epoch [5/100], train loss:0.0717\n",
      "test loss:0.0709\n",
      "epoch [6/100], train loss:0.0709\n",
      "test loss:0.0695\n",
      "epoch [7/100], train loss:0.0719\n",
      "test loss:0.0690\n",
      "epoch [8/100], train loss:0.0671\n",
      "test loss:0.0679\n",
      "epoch [9/100], train loss:0.0651\n",
      "test loss:0.0673\n",
      "epoch [10/100], train loss:0.0639\n",
      "test loss:0.0643\n",
      "epoch [11/100], train loss:0.0650\n",
      "test loss:0.0642\n",
      "epoch [12/100], train loss:0.0665\n",
      "test loss:0.0640\n",
      "epoch [13/100], train loss:0.0705\n",
      "test loss:0.0639\n",
      "epoch [14/100], train loss:0.0628\n",
      "test loss:0.0637\n",
      "epoch [15/100], train loss:0.0629\n",
      "test loss:0.0636\n",
      "epoch [16/100], train loss:0.0649\n",
      "test loss:0.0635\n",
      "epoch [17/100], train loss:0.0646\n",
      "test loss:0.0633\n",
      "epoch [18/100], train loss:0.0658\n",
      "test loss:0.0632\n",
      "epoch [19/100], train loss:0.0640\n",
      "test loss:0.0632\n",
      "epoch [20/100], train loss:0.0678\n",
      "test loss:0.0628\n",
      "epoch [21/100], train loss:0.0586\n",
      "test loss:0.0628\n",
      "epoch [22/100], train loss:0.0621\n",
      "test loss:0.0628\n",
      "epoch [23/100], train loss:0.0650\n",
      "test loss:0.0628\n",
      "epoch [24/100], train loss:0.0676\n",
      "test loss:0.0628\n",
      "epoch [25/100], train loss:0.0591\n",
      "test loss:0.0628\n",
      "epoch [26/100], train loss:0.0602\n",
      "test loss:0.0627\n",
      "epoch [27/100], train loss:0.0606\n",
      "test loss:0.0627\n",
      "epoch [28/100], train loss:0.0652\n",
      "test loss:0.0627\n",
      "epoch [29/100], train loss:0.0615\n",
      "test loss:0.0627\n",
      "epoch [30/100], train loss:0.0619\n",
      "test loss:0.0627\n",
      "epoch [31/100], train loss:0.0599\n",
      "test loss:0.0627\n",
      "epoch [32/100], train loss:0.0605\n",
      "test loss:0.0627\n",
      "epoch [33/100], train loss:0.0595\n",
      "test loss:0.0627\n",
      "epoch [34/100], train loss:0.0615\n",
      "test loss:0.0627\n",
      "epoch [35/100], train loss:0.0644\n",
      "test loss:0.0627\n",
      "epoch [36/100], train loss:0.0646\n",
      "test loss:0.0627\n",
      "epoch [37/100], train loss:0.0614\n",
      "test loss:0.0627\n",
      "epoch [38/100], train loss:0.0616\n",
      "test loss:0.0627\n",
      "epoch [39/100], train loss:0.0617\n",
      "test loss:0.0627\n",
      "epoch [40/100], train loss:0.0608\n",
      "test loss:0.0627\n",
      "epoch [41/100], train loss:0.0645\n",
      "test loss:0.0627\n",
      "epoch [42/100], train loss:0.0583\n",
      "test loss:0.0627\n",
      "epoch [43/100], train loss:0.0638\n",
      "test loss:0.0627\n",
      "epoch [44/100], train loss:0.0626\n",
      "test loss:0.0627\n",
      "epoch [45/100], train loss:0.0612\n",
      "test loss:0.0627\n",
      "epoch [46/100], train loss:0.0619\n",
      "test loss:0.0627\n",
      "epoch [47/100], train loss:0.0608\n",
      "test loss:0.0627\n",
      "epoch [48/100], train loss:0.0627\n",
      "test loss:0.0627\n",
      "epoch [49/100], train loss:0.0637\n",
      "test loss:0.0627\n",
      "epoch [50/100], train loss:0.0613\n",
      "test loss:0.0627\n",
      "epoch [51/100], train loss:0.0599\n",
      "test loss:0.0627\n",
      "epoch [52/100], train loss:0.0623\n",
      "test loss:0.0627\n",
      "epoch [53/100], train loss:0.0613\n",
      "test loss:0.0627\n",
      "epoch [54/100], train loss:0.0640\n",
      "test loss:0.0627\n",
      "epoch [55/100], train loss:0.0634\n",
      "test loss:0.0627\n",
      "epoch [56/100], train loss:0.0627\n",
      "test loss:0.0627\n",
      "epoch [57/100], train loss:0.0614\n",
      "test loss:0.0627\n",
      "epoch [58/100], train loss:0.0666\n",
      "test loss:0.0627\n",
      "epoch [59/100], train loss:0.0601\n",
      "test loss:0.0627\n",
      "epoch [60/100], train loss:0.0647\n",
      "test loss:0.0627\n",
      "epoch [61/100], train loss:0.0667\n",
      "test loss:0.0627\n",
      "epoch [62/100], train loss:0.0657\n",
      "test loss:0.0627\n",
      "epoch [63/100], train loss:0.0669\n",
      "test loss:0.0627\n",
      "epoch [64/100], train loss:0.0634\n",
      "test loss:0.0627\n",
      "epoch [65/100], train loss:0.0648\n",
      "test loss:0.0627\n",
      "epoch [66/100], train loss:0.0621\n",
      "test loss:0.0627\n",
      "epoch [67/100], train loss:0.0598\n",
      "test loss:0.0627\n",
      "epoch [68/100], train loss:0.0620\n",
      "test loss:0.0627\n",
      "epoch [69/100], train loss:0.0595\n",
      "test loss:0.0627\n",
      "epoch [70/100], train loss:0.0632\n",
      "test loss:0.0627\n",
      "epoch [71/100], train loss:0.0631\n",
      "test loss:0.0627\n",
      "epoch [72/100], train loss:0.0667\n",
      "test loss:0.0627\n",
      "epoch [73/100], train loss:0.0638\n",
      "test loss:0.0627\n",
      "epoch [74/100], train loss:0.0589\n",
      "test loss:0.0627\n",
      "epoch [75/100], train loss:0.0647\n",
      "test loss:0.0627\n",
      "epoch [76/100], train loss:0.0619\n",
      "test loss:0.0627\n",
      "epoch [77/100], train loss:0.0599\n",
      "test loss:0.0627\n",
      "epoch [78/100], train loss:0.0629\n",
      "test loss:0.0627\n",
      "epoch [79/100], train loss:0.0610\n",
      "test loss:0.0627\n",
      "epoch [80/100], train loss:0.0613\n",
      "test loss:0.0627\n",
      "epoch [81/100], train loss:0.0586\n",
      "test loss:0.0627\n",
      "epoch [82/100], train loss:0.0536\n",
      "test loss:0.0627\n",
      "epoch [83/100], train loss:0.0657\n",
      "test loss:0.0627\n",
      "epoch [84/100], train loss:0.0621\n",
      "test loss:0.0627\n",
      "epoch [85/100], train loss:0.0606\n",
      "test loss:0.0627\n",
      "epoch [86/100], train loss:0.0683\n",
      "test loss:0.0627\n",
      "epoch [87/100], train loss:0.0596\n",
      "test loss:0.0627\n",
      "epoch [88/100], train loss:0.0640\n",
      "test loss:0.0627\n",
      "epoch [89/100], train loss:0.0636\n",
      "test loss:0.0627\n",
      "epoch [90/100], train loss:0.0607\n",
      "test loss:0.0627\n",
      "epoch [91/100], train loss:0.0567\n",
      "test loss:0.0627\n",
      "epoch [92/100], train loss:0.0621\n",
      "test loss:0.0627\n",
      "epoch [93/100], train loss:0.0601\n",
      "test loss:0.0627\n",
      "epoch [94/100], train loss:0.0595\n",
      "test loss:0.0627\n",
      "epoch [95/100], train loss:0.0628\n",
      "test loss:0.0627\n",
      "epoch [96/100], train loss:0.0627\n",
      "test loss:0.0627\n",
      "epoch [97/100], train loss:0.0546\n",
      "test loss:0.0627\n",
      "epoch [98/100], train loss:0.0633\n",
      "test loss:0.0627\n",
      "epoch [99/100], train loss:0.0647\n",
      "test loss:0.0627\n",
      "epoch [100/100], train loss:0.0595\n",
      "test loss:0.0627\n",
      "bottleneck dimension: 6\n",
      "epoch [1/100], train loss:0.0885\n",
      "test loss:0.0881\n",
      "epoch [2/100], train loss:0.0778\n",
      "test loss:0.0725\n",
      "epoch [3/100], train loss:0.0659\n",
      "test loss:0.0675\n",
      "epoch [4/100], train loss:0.0674\n",
      "test loss:0.0645\n",
      "epoch [5/100], train loss:0.0563\n",
      "test loss:0.0625\n",
      "epoch [6/100], train loss:0.0579\n",
      "test loss:0.0610\n",
      "epoch [7/100], train loss:0.0572\n",
      "test loss:0.0596\n",
      "epoch [8/100], train loss:0.0575\n",
      "test loss:0.0588\n",
      "epoch [9/100], train loss:0.0585\n",
      "test loss:0.0583\n",
      "epoch [10/100], train loss:0.0544\n",
      "test loss:0.0544\n",
      "epoch [11/100], train loss:0.0522\n",
      "test loss:0.0542\n",
      "epoch [12/100], train loss:0.0533\n",
      "test loss:0.0540\n",
      "epoch [13/100], train loss:0.0515\n",
      "test loss:0.0539\n",
      "epoch [14/100], train loss:0.0556\n",
      "test loss:0.0537\n",
      "epoch [15/100], train loss:0.0562\n",
      "test loss:0.0536\n",
      "epoch [16/100], train loss:0.0522\n",
      "test loss:0.0535\n",
      "epoch [17/100], train loss:0.0576\n",
      "test loss:0.0533\n",
      "epoch [18/100], train loss:0.0492\n",
      "test loss:0.0532\n",
      "epoch [19/100], train loss:0.0501\n",
      "test loss:0.0531\n",
      "epoch [20/100], train loss:0.0546\n",
      "test loss:0.0527\n",
      "epoch [21/100], train loss:0.0510\n",
      "test loss:0.0527\n",
      "epoch [22/100], train loss:0.0520\n",
      "test loss:0.0527\n",
      "epoch [23/100], train loss:0.0521\n",
      "test loss:0.0527\n",
      "epoch [24/100], train loss:0.0500\n",
      "test loss:0.0527\n",
      "epoch [25/100], train loss:0.0537\n",
      "test loss:0.0526\n",
      "epoch [26/100], train loss:0.0509\n",
      "test loss:0.0526\n",
      "epoch [27/100], train loss:0.0543\n",
      "test loss:0.0526\n",
      "epoch [28/100], train loss:0.0516\n",
      "test loss:0.0526\n",
      "epoch [29/100], train loss:0.0516\n",
      "test loss:0.0526\n",
      "epoch [30/100], train loss:0.0516\n",
      "test loss:0.0526\n",
      "epoch [31/100], train loss:0.0512\n",
      "test loss:0.0526\n",
      "epoch [32/100], train loss:0.0521\n",
      "test loss:0.0526\n",
      "epoch [33/100], train loss:0.0522\n",
      "test loss:0.0526\n",
      "epoch [34/100], train loss:0.0490\n",
      "test loss:0.0526\n",
      "epoch [35/100], train loss:0.0521\n",
      "test loss:0.0526\n",
      "epoch [36/100], train loss:0.0535\n",
      "test loss:0.0526\n",
      "epoch [37/100], train loss:0.0545\n",
      "test loss:0.0525\n",
      "epoch [38/100], train loss:0.0529\n",
      "test loss:0.0525\n",
      "epoch [39/100], train loss:0.0547\n",
      "test loss:0.0525\n",
      "epoch [40/100], train loss:0.0479\n",
      "test loss:0.0525\n",
      "epoch [41/100], train loss:0.0507\n",
      "test loss:0.0525\n",
      "epoch [42/100], train loss:0.0519\n",
      "test loss:0.0525\n",
      "epoch [43/100], train loss:0.0491\n",
      "test loss:0.0525\n",
      "epoch [44/100], train loss:0.0521\n",
      "test loss:0.0525\n",
      "epoch [45/100], train loss:0.0539\n",
      "test loss:0.0525\n",
      "epoch [46/100], train loss:0.0531\n",
      "test loss:0.0525\n",
      "epoch [47/100], train loss:0.0517\n",
      "test loss:0.0525\n",
      "epoch [48/100], train loss:0.0524\n",
      "test loss:0.0525\n",
      "epoch [49/100], train loss:0.0563\n",
      "test loss:0.0525\n",
      "epoch [50/100], train loss:0.0561\n",
      "test loss:0.0525\n",
      "epoch [51/100], train loss:0.0525\n",
      "test loss:0.0525\n",
      "epoch [52/100], train loss:0.0519\n",
      "test loss:0.0525\n",
      "epoch [53/100], train loss:0.0534\n",
      "test loss:0.0525\n",
      "epoch [54/100], train loss:0.0531\n",
      "test loss:0.0525\n",
      "epoch [55/100], train loss:0.0554\n",
      "test loss:0.0525\n",
      "epoch [56/100], train loss:0.0498\n",
      "test loss:0.0525\n",
      "epoch [57/100], train loss:0.0549\n",
      "test loss:0.0525\n",
      "epoch [58/100], train loss:0.0515\n",
      "test loss:0.0525\n",
      "epoch [59/100], train loss:0.0516\n",
      "test loss:0.0525\n",
      "epoch [60/100], train loss:0.0530\n",
      "test loss:0.0525\n",
      "epoch [61/100], train loss:0.0523\n",
      "test loss:0.0525\n",
      "epoch [62/100], train loss:0.0532\n",
      "test loss:0.0525\n",
      "epoch [63/100], train loss:0.0521\n",
      "test loss:0.0525\n",
      "epoch [64/100], train loss:0.0542\n",
      "test loss:0.0525\n",
      "epoch [65/100], train loss:0.0484\n",
      "test loss:0.0525\n",
      "epoch [66/100], train loss:0.0495\n",
      "test loss:0.0525\n",
      "epoch [67/100], train loss:0.0500\n",
      "test loss:0.0525\n",
      "epoch [68/100], train loss:0.0494\n",
      "test loss:0.0525\n",
      "epoch [69/100], train loss:0.0522\n",
      "test loss:0.0525\n",
      "epoch [70/100], train loss:0.0501\n",
      "test loss:0.0525\n",
      "epoch [71/100], train loss:0.0537\n",
      "test loss:0.0525\n",
      "epoch [72/100], train loss:0.0511\n",
      "test loss:0.0525\n",
      "epoch [73/100], train loss:0.0501\n",
      "test loss:0.0525\n",
      "epoch [74/100], train loss:0.0547\n",
      "test loss:0.0525\n",
      "epoch [75/100], train loss:0.0532\n",
      "test loss:0.0525\n",
      "epoch [76/100], train loss:0.0569\n",
      "test loss:0.0525\n",
      "epoch [77/100], train loss:0.0485\n",
      "test loss:0.0525\n",
      "epoch [78/100], train loss:0.0539\n",
      "test loss:0.0525\n",
      "epoch [79/100], train loss:0.0537\n",
      "test loss:0.0525\n",
      "epoch [80/100], train loss:0.0517\n",
      "test loss:0.0525\n",
      "epoch [81/100], train loss:0.0525\n",
      "test loss:0.0525\n",
      "epoch [82/100], train loss:0.0531\n",
      "test loss:0.0525\n",
      "epoch [83/100], train loss:0.0498\n",
      "test loss:0.0525\n",
      "epoch [84/100], train loss:0.0501\n",
      "test loss:0.0525\n",
      "epoch [85/100], train loss:0.0550\n",
      "test loss:0.0525\n",
      "epoch [86/100], train loss:0.0501\n",
      "test loss:0.0525\n",
      "epoch [87/100], train loss:0.0535\n",
      "test loss:0.0525\n",
      "epoch [88/100], train loss:0.0508\n",
      "test loss:0.0525\n",
      "epoch [89/100], train loss:0.0529\n",
      "test loss:0.0525\n",
      "epoch [90/100], train loss:0.0543\n",
      "test loss:0.0525\n",
      "epoch [91/100], train loss:0.0536\n",
      "test loss:0.0525\n",
      "epoch [92/100], train loss:0.0512\n",
      "test loss:0.0525\n",
      "epoch [93/100], train loss:0.0516\n",
      "test loss:0.0525\n",
      "epoch [94/100], train loss:0.0499\n",
      "test loss:0.0525\n",
      "epoch [95/100], train loss:0.0501\n",
      "test loss:0.0525\n",
      "epoch [96/100], train loss:0.0533\n",
      "test loss:0.0525\n",
      "epoch [97/100], train loss:0.0505\n",
      "test loss:0.0525\n",
      "epoch [98/100], train loss:0.0527\n",
      "test loss:0.0525\n",
      "epoch [99/100], train loss:0.0519\n",
      "test loss:0.0525\n",
      "epoch [100/100], train loss:0.0544\n",
      "test loss:0.0525\n",
      "bottleneck dimension: 8\n",
      "epoch [1/100], train loss:0.0858\n",
      "test loss:0.0881\n",
      "epoch [2/100], train loss:0.0647\n",
      "test loss:0.0703\n",
      "epoch [3/100], train loss:0.0655\n",
      "test loss:0.0641\n",
      "epoch [4/100], train loss:0.0640\n",
      "test loss:0.0606\n",
      "epoch [5/100], train loss:0.0597\n",
      "test loss:0.0588\n",
      "epoch [6/100], train loss:0.0581\n",
      "test loss:0.0568\n",
      "epoch [7/100], train loss:0.0512\n",
      "test loss:0.0551\n",
      "epoch [8/100], train loss:0.0572\n",
      "test loss:0.0542\n",
      "epoch [9/100], train loss:0.0542\n",
      "test loss:0.0533\n",
      "epoch [10/100], train loss:0.0482\n",
      "test loss:0.0496\n",
      "epoch [11/100], train loss:0.0490\n",
      "test loss:0.0494\n",
      "epoch [12/100], train loss:0.0477\n",
      "test loss:0.0492\n",
      "epoch [13/100], train loss:0.0500\n",
      "test loss:0.0490\n",
      "epoch [14/100], train loss:0.0518\n",
      "test loss:0.0489\n",
      "epoch [15/100], train loss:0.0474\n",
      "test loss:0.0487\n",
      "epoch [16/100], train loss:0.0501\n",
      "test loss:0.0486\n",
      "epoch [17/100], train loss:0.0505\n",
      "test loss:0.0484\n",
      "epoch [18/100], train loss:0.0489\n",
      "test loss:0.0483\n",
      "epoch [19/100], train loss:0.0476\n",
      "test loss:0.0482\n",
      "epoch [20/100], train loss:0.0463\n",
      "test loss:0.0478\n",
      "epoch [21/100], train loss:0.0472\n",
      "test loss:0.0477\n",
      "epoch [22/100], train loss:0.0481\n",
      "test loss:0.0477\n",
      "epoch [23/100], train loss:0.0464\n",
      "test loss:0.0477\n",
      "epoch [24/100], train loss:0.0466\n",
      "test loss:0.0477\n",
      "epoch [25/100], train loss:0.0463\n",
      "test loss:0.0477\n",
      "epoch [26/100], train loss:0.0437\n",
      "test loss:0.0476\n",
      "epoch [27/100], train loss:0.0458\n",
      "test loss:0.0476\n",
      "epoch [28/100], train loss:0.0488\n",
      "test loss:0.0476\n",
      "epoch [29/100], train loss:0.0466\n",
      "test loss:0.0476\n",
      "epoch [30/100], train loss:0.0487\n",
      "test loss:0.0476\n",
      "epoch [31/100], train loss:0.0511\n",
      "test loss:0.0476\n",
      "epoch [32/100], train loss:0.0451\n",
      "test loss:0.0476\n",
      "epoch [33/100], train loss:0.0467\n",
      "test loss:0.0476\n",
      "epoch [34/100], train loss:0.0466\n",
      "test loss:0.0476\n",
      "epoch [35/100], train loss:0.0476\n",
      "test loss:0.0476\n",
      "epoch [36/100], train loss:0.0453\n",
      "test loss:0.0476\n",
      "epoch [37/100], train loss:0.0488\n",
      "test loss:0.0476\n",
      "epoch [38/100], train loss:0.0465\n",
      "test loss:0.0476\n",
      "epoch [39/100], train loss:0.0494\n",
      "test loss:0.0476\n",
      "epoch [40/100], train loss:0.0505\n",
      "test loss:0.0476\n",
      "epoch [41/100], train loss:0.0493\n",
      "test loss:0.0476\n",
      "epoch [42/100], train loss:0.0479\n",
      "test loss:0.0475\n",
      "epoch [43/100], train loss:0.0483\n",
      "test loss:0.0475\n",
      "epoch [44/100], train loss:0.0492\n",
      "test loss:0.0475\n",
      "epoch [45/100], train loss:0.0490\n",
      "test loss:0.0475\n",
      "epoch [46/100], train loss:0.0457\n",
      "test loss:0.0475\n",
      "epoch [47/100], train loss:0.0450\n",
      "test loss:0.0475\n",
      "epoch [48/100], train loss:0.0466\n",
      "test loss:0.0475\n",
      "epoch [49/100], train loss:0.0512\n",
      "test loss:0.0475\n",
      "epoch [50/100], train loss:0.0453\n",
      "test loss:0.0475\n",
      "epoch [51/100], train loss:0.0445\n",
      "test loss:0.0475\n",
      "epoch [52/100], train loss:0.0467\n",
      "test loss:0.0475\n",
      "epoch [53/100], train loss:0.0486\n",
      "test loss:0.0475\n",
      "epoch [54/100], train loss:0.0503\n",
      "test loss:0.0475\n",
      "epoch [55/100], train loss:0.0464\n",
      "test loss:0.0475\n",
      "epoch [56/100], train loss:0.0474\n",
      "test loss:0.0475\n",
      "epoch [57/100], train loss:0.0482\n",
      "test loss:0.0475\n",
      "epoch [58/100], train loss:0.0472\n",
      "test loss:0.0475\n",
      "epoch [59/100], train loss:0.0466\n",
      "test loss:0.0475\n",
      "epoch [60/100], train loss:0.0449\n",
      "test loss:0.0475\n",
      "epoch [61/100], train loss:0.0467\n",
      "test loss:0.0475\n",
      "epoch [62/100], train loss:0.0466\n",
      "test loss:0.0475\n",
      "epoch [63/100], train loss:0.0469\n",
      "test loss:0.0475\n",
      "epoch [64/100], train loss:0.0476\n",
      "test loss:0.0475\n",
      "epoch [65/100], train loss:0.0466\n",
      "test loss:0.0475\n",
      "epoch [66/100], train loss:0.0457\n",
      "test loss:0.0475\n",
      "epoch [67/100], train loss:0.0488\n",
      "test loss:0.0475\n",
      "epoch [68/100], train loss:0.0477\n",
      "test loss:0.0475\n",
      "epoch [69/100], train loss:0.0476\n",
      "test loss:0.0475\n",
      "epoch [70/100], train loss:0.0461\n",
      "test loss:0.0475\n",
      "epoch [71/100], train loss:0.0459\n",
      "test loss:0.0475\n",
      "epoch [72/100], train loss:0.0470\n",
      "test loss:0.0475\n",
      "epoch [73/100], train loss:0.0466\n",
      "test loss:0.0475\n",
      "epoch [74/100], train loss:0.0454\n",
      "test loss:0.0475\n",
      "epoch [75/100], train loss:0.0480\n",
      "test loss:0.0475\n",
      "epoch [76/100], train loss:0.0504\n",
      "test loss:0.0475\n",
      "epoch [77/100], train loss:0.0479\n",
      "test loss:0.0475\n",
      "epoch [78/100], train loss:0.0488\n",
      "test loss:0.0475\n",
      "epoch [79/100], train loss:0.0469\n",
      "test loss:0.0475\n",
      "epoch [80/100], train loss:0.0475\n",
      "test loss:0.0475\n",
      "epoch [81/100], train loss:0.0518\n",
      "test loss:0.0475\n",
      "epoch [82/100], train loss:0.0473\n",
      "test loss:0.0475\n",
      "epoch [83/100], train loss:0.0494\n",
      "test loss:0.0475\n",
      "epoch [84/100], train loss:0.0478\n",
      "test loss:0.0475\n",
      "epoch [85/100], train loss:0.0439\n",
      "test loss:0.0475\n",
      "epoch [86/100], train loss:0.0473\n",
      "test loss:0.0475\n",
      "epoch [87/100], train loss:0.0477\n",
      "test loss:0.0475\n",
      "epoch [88/100], train loss:0.0480\n",
      "test loss:0.0475\n",
      "epoch [89/100], train loss:0.0478\n",
      "test loss:0.0475\n",
      "epoch [90/100], train loss:0.0479\n",
      "test loss:0.0475\n",
      "epoch [91/100], train loss:0.0485\n",
      "test loss:0.0475\n",
      "epoch [92/100], train loss:0.0472\n",
      "test loss:0.0475\n",
      "epoch [93/100], train loss:0.0467\n",
      "test loss:0.0475\n",
      "epoch [94/100], train loss:0.0485\n",
      "test loss:0.0475\n",
      "epoch [95/100], train loss:0.0460\n",
      "test loss:0.0475\n",
      "epoch [96/100], train loss:0.0499\n",
      "test loss:0.0475\n",
      "epoch [97/100], train loss:0.0431\n",
      "test loss:0.0475\n",
      "epoch [98/100], train loss:0.0472\n",
      "test loss:0.0475\n",
      "epoch [99/100], train loss:0.0456\n",
      "test loss:0.0475\n",
      "epoch [100/100], train loss:0.0458\n",
      "test loss:0.0475\n",
      "bottleneck dimension: 12\n",
      "epoch [1/100], train loss:0.0902\n",
      "test loss:0.0890\n",
      "epoch [2/100], train loss:0.0695\n",
      "test loss:0.0696\n",
      "epoch [3/100], train loss:0.0635\n",
      "test loss:0.0620\n",
      "epoch [4/100], train loss:0.0588\n",
      "test loss:0.0573\n",
      "epoch [5/100], train loss:0.0526\n",
      "test loss:0.0537\n",
      "epoch [6/100], train loss:0.0533\n",
      "test loss:0.0515\n",
      "epoch [7/100], train loss:0.0503\n",
      "test loss:0.0498\n",
      "epoch [8/100], train loss:0.0484\n",
      "test loss:0.0485\n",
      "epoch [9/100], train loss:0.0443\n",
      "test loss:0.0472\n",
      "epoch [10/100], train loss:0.0427\n",
      "test loss:0.0435\n",
      "epoch [11/100], train loss:0.0404\n",
      "test loss:0.0432\n",
      "epoch [12/100], train loss:0.0422\n",
      "test loss:0.0430\n",
      "epoch [13/100], train loss:0.0426\n",
      "test loss:0.0428\n",
      "epoch [14/100], train loss:0.0441\n",
      "test loss:0.0426\n",
      "epoch [15/100], train loss:0.0436\n",
      "test loss:0.0425\n",
      "epoch [16/100], train loss:0.0429\n",
      "test loss:0.0423\n",
      "epoch [17/100], train loss:0.0421\n",
      "test loss:0.0421\n",
      "epoch [18/100], train loss:0.0438\n",
      "test loss:0.0420\n",
      "epoch [19/100], train loss:0.0408\n",
      "test loss:0.0418\n",
      "epoch [20/100], train loss:0.0426\n",
      "test loss:0.0414\n",
      "epoch [21/100], train loss:0.0431\n",
      "test loss:0.0414\n",
      "epoch [22/100], train loss:0.0403\n",
      "test loss:0.0413\n",
      "epoch [23/100], train loss:0.0409\n",
      "test loss:0.0413\n",
      "epoch [24/100], train loss:0.0448\n",
      "test loss:0.0413\n",
      "epoch [25/100], train loss:0.0435\n",
      "test loss:0.0413\n",
      "epoch [26/100], train loss:0.0411\n",
      "test loss:0.0413\n",
      "epoch [27/100], train loss:0.0398\n",
      "test loss:0.0412\n",
      "epoch [28/100], train loss:0.0413\n",
      "test loss:0.0412\n",
      "epoch [29/100], train loss:0.0418\n",
      "test loss:0.0412\n",
      "epoch [30/100], train loss:0.0388\n",
      "test loss:0.0412\n",
      "epoch [31/100], train loss:0.0395\n",
      "test loss:0.0412\n",
      "epoch [32/100], train loss:0.0434\n",
      "test loss:0.0412\n",
      "epoch [33/100], train loss:0.0404\n",
      "test loss:0.0412\n",
      "epoch [34/100], train loss:0.0394\n",
      "test loss:0.0412\n",
      "epoch [35/100], train loss:0.0410\n",
      "test loss:0.0412\n",
      "epoch [36/100], train loss:0.0428\n",
      "test loss:0.0412\n",
      "epoch [37/100], train loss:0.0407\n",
      "test loss:0.0412\n",
      "epoch [38/100], train loss:0.0427\n",
      "test loss:0.0412\n",
      "epoch [39/100], train loss:0.0400\n",
      "test loss:0.0412\n",
      "epoch [40/100], train loss:0.0394\n",
      "test loss:0.0412\n",
      "epoch [41/100], train loss:0.0393\n",
      "test loss:0.0412\n",
      "epoch [42/100], train loss:0.0408\n",
      "test loss:0.0412\n",
      "epoch [43/100], train loss:0.0438\n",
      "test loss:0.0412\n",
      "epoch [44/100], train loss:0.0405\n",
      "test loss:0.0412\n",
      "epoch [45/100], train loss:0.0398\n",
      "test loss:0.0412\n",
      "epoch [46/100], train loss:0.0424\n",
      "test loss:0.0412\n",
      "epoch [47/100], train loss:0.0398\n",
      "test loss:0.0412\n",
      "epoch [48/100], train loss:0.0429\n",
      "test loss:0.0412\n",
      "epoch [49/100], train loss:0.0422\n",
      "test loss:0.0412\n",
      "epoch [50/100], train loss:0.0416\n",
      "test loss:0.0412\n",
      "epoch [51/100], train loss:0.0428\n",
      "test loss:0.0412\n",
      "epoch [52/100], train loss:0.0404\n",
      "test loss:0.0412\n",
      "epoch [53/100], train loss:0.0409\n",
      "test loss:0.0412\n",
      "epoch [54/100], train loss:0.0397\n",
      "test loss:0.0412\n",
      "epoch [55/100], train loss:0.0434\n",
      "test loss:0.0412\n",
      "epoch [56/100], train loss:0.0427\n",
      "test loss:0.0412\n",
      "epoch [57/100], train loss:0.0419\n",
      "test loss:0.0412\n",
      "epoch [58/100], train loss:0.0418\n",
      "test loss:0.0412\n",
      "epoch [59/100], train loss:0.0411\n",
      "test loss:0.0412\n",
      "epoch [60/100], train loss:0.0404\n",
      "test loss:0.0412\n",
      "epoch [61/100], train loss:0.0389\n",
      "test loss:0.0412\n",
      "epoch [62/100], train loss:0.0420\n",
      "test loss:0.0412\n",
      "epoch [63/100], train loss:0.0445\n",
      "test loss:0.0412\n",
      "epoch [64/100], train loss:0.0401\n",
      "test loss:0.0412\n",
      "epoch [65/100], train loss:0.0408\n",
      "test loss:0.0412\n",
      "epoch [66/100], train loss:0.0403\n",
      "test loss:0.0412\n",
      "epoch [67/100], train loss:0.0393\n",
      "test loss:0.0412\n",
      "epoch [68/100], train loss:0.0389\n",
      "test loss:0.0412\n",
      "epoch [69/100], train loss:0.0413\n",
      "test loss:0.0412\n",
      "epoch [70/100], train loss:0.0410\n",
      "test loss:0.0412\n",
      "epoch [71/100], train loss:0.0426\n",
      "test loss:0.0412\n",
      "epoch [72/100], train loss:0.0399\n",
      "test loss:0.0412\n",
      "epoch [73/100], train loss:0.0390\n",
      "test loss:0.0412\n",
      "epoch [74/100], train loss:0.0410\n",
      "test loss:0.0412\n",
      "epoch [75/100], train loss:0.0387\n",
      "test loss:0.0412\n",
      "epoch [76/100], train loss:0.0409\n",
      "test loss:0.0412\n",
      "epoch [77/100], train loss:0.0405\n",
      "test loss:0.0412\n",
      "epoch [78/100], train loss:0.0401\n",
      "test loss:0.0412\n",
      "epoch [79/100], train loss:0.0413\n",
      "test loss:0.0412\n",
      "epoch [80/100], train loss:0.0428\n",
      "test loss:0.0412\n",
      "epoch [81/100], train loss:0.0415\n",
      "test loss:0.0412\n",
      "epoch [82/100], train loss:0.0413\n",
      "test loss:0.0412\n",
      "epoch [83/100], train loss:0.0398\n",
      "test loss:0.0412\n",
      "epoch [84/100], train loss:0.0427\n",
      "test loss:0.0412\n",
      "epoch [85/100], train loss:0.0416\n",
      "test loss:0.0412\n",
      "epoch [86/100], train loss:0.0405\n",
      "test loss:0.0412\n",
      "epoch [87/100], train loss:0.0411\n",
      "test loss:0.0412\n",
      "epoch [88/100], train loss:0.0441\n",
      "test loss:0.0412\n",
      "epoch [89/100], train loss:0.0415\n",
      "test loss:0.0412\n",
      "epoch [90/100], train loss:0.0430\n",
      "test loss:0.0412\n",
      "epoch [91/100], train loss:0.0401\n",
      "test loss:0.0412\n",
      "epoch [92/100], train loss:0.0397\n",
      "test loss:0.0412\n",
      "epoch [93/100], train loss:0.0413\n",
      "test loss:0.0412\n",
      "epoch [94/100], train loss:0.0412\n",
      "test loss:0.0412\n",
      "epoch [95/100], train loss:0.0407\n",
      "test loss:0.0412\n",
      "epoch [96/100], train loss:0.0428\n",
      "test loss:0.0412\n",
      "epoch [97/100], train loss:0.0394\n",
      "test loss:0.0412\n",
      "epoch [98/100], train loss:0.0404\n",
      "test loss:0.0412\n",
      "epoch [99/100], train loss:0.0411\n",
      "test loss:0.0412\n",
      "epoch [100/100], train loss:0.0400\n",
      "test loss:0.0412\n",
      "bottleneck dimension: 16\n",
      "epoch [1/100], train loss:0.0918\n",
      "test loss:0.0899\n",
      "epoch [2/100], train loss:0.0697\n",
      "test loss:0.0705\n",
      "epoch [3/100], train loss:0.0649\n",
      "test loss:0.0633\n",
      "epoch [4/100], train loss:0.0582\n",
      "test loss:0.0587\n",
      "epoch [5/100], train loss:0.0527\n",
      "test loss:0.0546\n",
      "epoch [6/100], train loss:0.0525\n",
      "test loss:0.0524\n",
      "epoch [7/100], train loss:0.0549\n",
      "test loss:0.0504\n",
      "epoch [8/100], train loss:0.0488\n",
      "test loss:0.0482\n",
      "epoch [9/100], train loss:0.0500\n",
      "test loss:0.0469\n",
      "epoch [10/100], train loss:0.0437\n",
      "test loss:0.0435\n",
      "epoch [11/100], train loss:0.0414\n",
      "test loss:0.0432\n",
      "epoch [12/100], train loss:0.0432\n",
      "test loss:0.0430\n",
      "epoch [13/100], train loss:0.0415\n",
      "test loss:0.0428\n",
      "epoch [14/100], train loss:0.0429\n",
      "test loss:0.0426\n",
      "epoch [15/100], train loss:0.0433\n",
      "test loss:0.0424\n",
      "epoch [16/100], train loss:0.0410\n",
      "test loss:0.0422\n",
      "epoch [17/100], train loss:0.0429\n",
      "test loss:0.0420\n",
      "epoch [18/100], train loss:0.0418\n",
      "test loss:0.0418\n",
      "epoch [19/100], train loss:0.0402\n",
      "test loss:0.0416\n",
      "epoch [20/100], train loss:0.0424\n",
      "test loss:0.0412\n",
      "epoch [21/100], train loss:0.0430\n",
      "test loss:0.0412\n",
      "epoch [22/100], train loss:0.0389\n",
      "test loss:0.0412\n",
      "epoch [23/100], train loss:0.0401\n",
      "test loss:0.0412\n",
      "epoch [24/100], train loss:0.0401\n",
      "test loss:0.0411\n",
      "epoch [25/100], train loss:0.0412\n",
      "test loss:0.0411\n",
      "epoch [26/100], train loss:0.0418\n",
      "test loss:0.0411\n",
      "epoch [27/100], train loss:0.0416\n",
      "test loss:0.0411\n",
      "epoch [28/100], train loss:0.0392\n",
      "test loss:0.0411\n",
      "epoch [29/100], train loss:0.0424\n",
      "test loss:0.0410\n",
      "epoch [30/100], train loss:0.0426\n",
      "test loss:0.0410\n",
      "epoch [31/100], train loss:0.0430\n",
      "test loss:0.0410\n",
      "epoch [32/100], train loss:0.0412\n",
      "test loss:0.0410\n",
      "epoch [33/100], train loss:0.0406\n",
      "test loss:0.0410\n",
      "epoch [34/100], train loss:0.0412\n",
      "test loss:0.0410\n",
      "epoch [35/100], train loss:0.0426\n",
      "test loss:0.0410\n",
      "epoch [36/100], train loss:0.0417\n",
      "test loss:0.0410\n",
      "epoch [37/100], train loss:0.0418\n",
      "test loss:0.0410\n",
      "epoch [38/100], train loss:0.0397\n",
      "test loss:0.0410\n",
      "epoch [39/100], train loss:0.0405\n",
      "test loss:0.0410\n",
      "epoch [40/100], train loss:0.0400\n",
      "test loss:0.0410\n",
      "epoch [41/100], train loss:0.0419\n",
      "test loss:0.0410\n",
      "epoch [42/100], train loss:0.0407\n",
      "test loss:0.0410\n",
      "epoch [43/100], train loss:0.0419\n",
      "test loss:0.0410\n",
      "epoch [44/100], train loss:0.0394\n",
      "test loss:0.0410\n",
      "epoch [45/100], train loss:0.0426\n",
      "test loss:0.0410\n",
      "epoch [46/100], train loss:0.0418\n",
      "test loss:0.0410\n",
      "epoch [47/100], train loss:0.0411\n",
      "test loss:0.0410\n",
      "epoch [48/100], train loss:0.0414\n",
      "test loss:0.0410\n",
      "epoch [49/100], train loss:0.0435\n",
      "test loss:0.0410\n",
      "epoch [50/100], train loss:0.0377\n",
      "test loss:0.0410\n",
      "epoch [51/100], train loss:0.0398\n",
      "test loss:0.0410\n",
      "epoch [52/100], train loss:0.0416\n",
      "test loss:0.0410\n",
      "epoch [53/100], train loss:0.0416\n",
      "test loss:0.0410\n",
      "epoch [54/100], train loss:0.0430\n",
      "test loss:0.0410\n",
      "epoch [55/100], train loss:0.0408\n",
      "test loss:0.0410\n",
      "epoch [56/100], train loss:0.0442\n",
      "test loss:0.0410\n",
      "epoch [57/100], train loss:0.0408\n",
      "test loss:0.0410\n",
      "epoch [58/100], train loss:0.0381\n",
      "test loss:0.0410\n",
      "epoch [59/100], train loss:0.0423\n",
      "test loss:0.0410\n",
      "epoch [60/100], train loss:0.0378\n",
      "test loss:0.0410\n",
      "epoch [61/100], train loss:0.0405\n",
      "test loss:0.0410\n",
      "epoch [62/100], train loss:0.0421\n",
      "test loss:0.0410\n",
      "epoch [63/100], train loss:0.0396\n",
      "test loss:0.0410\n",
      "epoch [64/100], train loss:0.0414\n",
      "test loss:0.0410\n",
      "epoch [65/100], train loss:0.0393\n",
      "test loss:0.0410\n",
      "epoch [66/100], train loss:0.0423\n",
      "test loss:0.0410\n",
      "epoch [67/100], train loss:0.0441\n",
      "test loss:0.0410\n",
      "epoch [68/100], train loss:0.0406\n",
      "test loss:0.0410\n",
      "epoch [69/100], train loss:0.0398\n",
      "test loss:0.0410\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_bottleneck_classic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmilestones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dim_lst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mvalidloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmnist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GITs/nsf_eager_24/malgorzata_workspace/inn-data-gen/functionalities/trainer.py:352\u001b[0m, in \u001b[0;36mtrain_bottleneck_classic\u001b[0;34m(num_epoch, get_model, modelname, milestones, latent_dim_lst, trainloader, validloader, testloader, lr_init, device, mnist)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epoch):\n\u001b[1;32m    351\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 352\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m trainloader:\n\u001b[1;32m    353\u001b[0m         img, _ \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mnist:\n",
      "File \u001b[0;32m~/.conda/envs/venv-p310-torch210-abx4kb/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/venv-p310-torch210-abx4kb/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/venv-p310-torch210-abx4kb/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/venv-p310-torch210-abx4kb/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/venv-p310-torch210-abx4kb/lib/python3.10/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/.conda/envs/venv-p310-torch210-abx4kb/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.conda/envs/venv-p310-torch210-abx4kb/lib/python3.10/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/venv-p310-torch210-abx4kb/lib/python3.10/site-packages/torchvision/transforms/functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tr.train_bottleneck_classic(num_epoch, get_model, modelname, milestones, latent_dim_lst, trainloader,\n",
    "                     validloader, testloader, lr_init=lr_init, device=device, mnist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Reconstruction and Difference Images Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_img = 100\n",
    "grid_row_size = 10\n",
    "\n",
    "img, label = next(iter(testloader))\n",
    "img = img.view(img.size(0), -1)\n",
    "img = Variable(img).cuda()\n",
    "\n",
    "for i in latent_dim_lst:\n",
    "    print('bottleneck dimension: {}'.format(i))\n",
    "    model = fm.load_model('{}_{}'.format(modelname, i))\n",
    "    output = model(img)\n",
    "\n",
    "    \n",
    "    original = pl.to_img(img.cpu().data, [1, 28, 28]) \n",
    "    pic = pl.to_img(output.cpu().data, [1, 28, 28])\n",
    "\n",
    "    print(\"Original Image:\")\n",
    "    pl.imshow(torchvision.utils.make_grid(original[:num_img].detach(), grid_row_size), filename=\"com_classic_mnist_{}_original\".format(i))\n",
    "    print(\"Reconstructed Image:\")\n",
    "    pl.imshow(torchvision.utils.make_grid(pic[:num_img].detach(), grid_row_size), filename=\"com_classic_mnist_{}_reconstructed\".format(i))\n",
    "    print(\"Difference:\")\n",
    "    diff_img = (original - pic + 1) / 2\n",
    "    pl.imshow(torchvision.utils.make_grid(diff_img[:num_img].detach(), grid_row_size), filename=\"com_classic_mnist_{}_difference\".format(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Recontruction Loss against Bottleneck Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train, test = fm.load_variable(\"{}_bottleneck\".format(modelname))\n",
    "y = [train, test]\n",
    "x = []\n",
    "for loss in y:\n",
    "    x.append([1, 2, 3, 4, 6, 8, 12, 16, 24, 32, 48, 64])\n",
    "\n",
    "pl.plot(latent_dim_lst, y, 'bottleneck size', 'loss', ['train', 'test'], 'Train & Test Reconstruction Loss History', 'loss_l1_bottleneck') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat for classic 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretraining Setup\n",
    "num_epoch = 100\n",
    "batch_size = 128\n",
    "lr_init = 1e-3\n",
    "milestones = [10 * x for x in range(1, 11)]\n",
    "latent_dim_lst = [1, 2, 3, 4, 6, 8, 12, 16, 24, 32, 48, 64]\n",
    "number_dev = 0\n",
    "get_model = mnist.mnist_autoencoder_1024\n",
    "modelname = \"mnist_classic_1024_bottleneck\"\n",
    "\n",
    "device = gpu.get_device(number_dev)\n",
    "print(device)\n",
    "\n",
    "trainset, testset, classes = dl.load_mnist()\n",
    "trainloader, validloader, testloader = dl.make_dataloaders(trainset, testset, batch_size)\n",
    "\n",
    "# Training\n",
    "tr.train_bottleneck_classic(num_epoch, get_model, modelname, milestones, latent_dim_lst, trainloader,\n",
    "                     validloader, testloader, lr_init=lr_init, device=device, mnist=True)\n",
    "\n",
    "# Plot Reconstruction and Difference Images Examples\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_img = 100\n",
    "grid_row_size = 10\n",
    "\n",
    "img, label = next(iter(testloader))\n",
    "img = img.view(img.size(0), -1)\n",
    "img = Variable(img).cuda()\n",
    "\n",
    "for i in latent_dim_lst:\n",
    "    print('bottleneck dimension: {}'.format(i))\n",
    "    model = fm.load_model('{}_{}'.format(modelname, i))\n",
    "    output = model(img)\n",
    "\n",
    "    \n",
    "    original = pl.to_img(img.cpu().data, [1, 28, 28]) \n",
    "    pic = pl.to_img(output.cpu().data, [1, 28, 28])\n",
    "\n",
    "    print(\"Original Image:\")\n",
    "    pl.imshow(torchvision.utils.make_grid(original[:num_img].detach(), grid_row_size), filename=\"com_classic_mnist_{}_original\".format(i))\n",
    "    print(\"Reconstructed Image:\")\n",
    "    pl.imshow(torchvision.utils.make_grid(pic[:num_img].detach(), grid_row_size), filename=\"com_classic_mnist_{}_reconstructed\".format(i))\n",
    "    print(\"Difference:\")\n",
    "    diff_img = (original - pic + 1) / 2\n",
    "    pl.imshow(torchvision.utils.make_grid(diff_img[:num_img].detach(), grid_row_size), filename=\"com_classic_mnist_{}_difference\".format(i))\n",
    "    \n",
    "# Plot Recontruction Loss against Bottleneck Size\n",
    "train, test = fm.load_variable(\"{}_bottleneck\".format(modelname))\n",
    "y = [train, test]\n",
    "x = []\n",
    "for loss in y:\n",
    "    x.append([1, 2, 3, 4, 6, 8, 12, 16, 24, 32, 48, 64])\n",
    "\n",
    "pl.plot(latent_dim_lst, y, 'bottleneck size', 'loss', ['train', 'test'], 'Train & Test Reconstruction Loss History', 'loss_l1_bottleneck') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat for classic 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretraining Setup\n",
    "num_epoch = 100\n",
    "batch_size = 128\n",
    "lr_init = 1e-3\n",
    "milestones = [10 * x for x in range(1, 11)]\n",
    "latent_dim_lst = [1, 2, 3, 4, 6, 8, 12, 16, 24, 32, 48, 64]\n",
    "number_dev = 0\n",
    "get_model = mnist.mnist_autoencoder_2048\n",
    "modelname = \"mnist_classic_2048_bottleneck\"\n",
    "\n",
    "device = gpu.get_device(number_dev)\n",
    "print(device)\n",
    "\n",
    "trainset, testset, classes = dl.load_mnist()\n",
    "trainloader, validloader, testloader = dl.make_dataloaders(trainset, testset, batch_size)\n",
    "\n",
    "# Training\n",
    "tr.train_bottleneck_classic(num_epoch, get_model, modelname, milestones, latent_dim_lst, trainloader,\n",
    "                     validloader, testloader, lr_init=lr_init, device=device, mnist=True)\n",
    "\n",
    "# Plot Reconstruction and Difference Images Examples\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_img = 100\n",
    "grid_row_size = 10\n",
    "\n",
    "img, label = next(iter(testloader))\n",
    "img = img.view(img.size(0), -1)\n",
    "img = Variable(img).cuda()\n",
    "\n",
    "for i in latent_dim_lst:\n",
    "    print('bottleneck dimension: {}'.format(i))\n",
    "    model = fm.load_model('{}_{}'.format(modelname, i))\n",
    "    output = model(img)\n",
    "\n",
    "    \n",
    "    original = pl.to_img(img.cpu().data, [1, 28, 28]) \n",
    "    pic = pl.to_img(output.cpu().data, [1, 28, 28])\n",
    "\n",
    "    print(\"Original Image:\")\n",
    "    pl.imshow(torchvision.utils.make_grid(original[:num_img].detach(), grid_row_size), filename=\"com_classic_mnist_{}_original\".format(i))\n",
    "    print(\"Reconstructed Image:\")\n",
    "    pl.imshow(torchvision.utils.make_grid(pic[:num_img].detach(), grid_row_size), filename=\"com_classic_mnist_{}_reconstructed\".format(i))\n",
    "    print(\"Difference:\")\n",
    "    diff_img = (original - pic + 1) / 2\n",
    "    pl.imshow(torchvision.utils.make_grid(diff_img[:num_img].detach(), grid_row_size), filename=\"com_classic_mnist_{}_difference\".format(i))\n",
    "    \n",
    "# Plot Recontruction Loss against Bottleneck Size\n",
    "train, test = fm.load_variable(\"{}_bottleneck\".format(modelname))\n",
    "y = [train, test]\n",
    "x = []\n",
    "for loss in y:\n",
    "    x.append([1, 2, 3, 4, 6, 8, 12, 16, 24, 32, 48, 64])\n",
    "\n",
    "pl.plot(latent_dim_lst, y, 'bottleneck size', 'loss', ['train', 'test'], 'Train & Test Reconstruction Loss History', 'loss_l1_bottleneck') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat for deep classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretraining Setup\n",
    "num_epoch = 100\n",
    "batch_size = 128\n",
    "lr_init = 1e-3\n",
    "milestones = [10 * x for x in range(1, 11)]\n",
    "latent_dim_lst = [1, 2, 3, 4, 6, 8, 12, 16, 24, 32, 48, 64]\n",
    "number_dev = 0\n",
    "get_model = mnist.mnist_autoencoder_deep_1024\n",
    "modelname = \"mnist_classic_deep_bottleneck\"\n",
    "\n",
    "device = gpu.get_device(number_dev)\n",
    "print(device)\n",
    "\n",
    "trainset, testset, classes = dl.load_mnist()\n",
    "trainloader, validloader, testloader = dl.make_dataloaders(trainset, testset, batch_size)\n",
    "\n",
    "# Training\n",
    "tr.train_bottleneck_classic(num_epoch, get_model, modelname, milestones, latent_dim_lst, trainloader,\n",
    "                     validloader, testloader, lr_init=lr_init, device=device, mnist=True)\n",
    "\n",
    "# Plot Reconstruction and Difference Images Examples\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_img = 100\n",
    "grid_row_size = 10\n",
    "\n",
    "img, label = next(iter(testloader))\n",
    "img = img.view(img.size(0), -1)\n",
    "img = Variable(img).cuda()\n",
    "\n",
    "for i in latent_dim_lst:\n",
    "    print('bottleneck dimension: {}'.format(i))\n",
    "    model = fm.load_model('{}_{}'.format(modelname, i))\n",
    "    output = model(img)\n",
    "\n",
    "    \n",
    "    original = pl.to_img(img.cpu().data, [1, 28, 28]) \n",
    "    pic = pl.to_img(output.cpu().data, [1, 28, 28])\n",
    "\n",
    "    print(\"Original Image:\")\n",
    "    pl.imshow(torchvision.utils.make_grid(original[:num_img].detach(), grid_row_size), filename=\"com_classic_mnist_{}_original\".format(i))\n",
    "    print(\"Reconstructed Image:\")\n",
    "    pl.imshow(torchvision.utils.make_grid(pic[:num_img].detach(), grid_row_size), filename=\"com_classic_mnist_{}_reconstructed\".format(i))\n",
    "    print(\"Difference:\")\n",
    "    diff_img = (original - pic + 1) / 2\n",
    "    pl.imshow(torchvision.utils.make_grid(diff_img[:num_img].detach(), grid_row_size), filename=\"com_classic_mnist_{}_difference\".format(i))\n",
    "    \n",
    "# Plot Recontruction Loss against Bottleneck Size\n",
    "train, test = fm.load_variable(\"{}_bottleneck\".format(modelname))\n",
    "y = [train, test]\n",
    "x = []\n",
    "for loss in y:\n",
    "    x.append([1, 2, 3, 4, 6, 8, 12, 16, 24, 32, 48, 64])\n",
    "\n",
    "pl.plot(latent_dim_lst, y, 'bottleneck size', 'loss', ['train', 'test'], 'Train & Test Reconstruction Loss History', 'loss_l1_bottleneck') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-p310-torch210-abx4kb",
   "language": "python",
   "name": "venv-p310-torch210-abx4kb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
